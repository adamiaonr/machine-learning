\section{Semi-Supervised Learning}
\label{subsec:semi-supervised}

%We now provide a brief and informal description of the main rationale behind SSL classification and 
%then develop on a particular class of methods --- Generative Models --- 
%the basis of the text classification models studied in this paper.\vertbreak

In purely Supervised Learning problems, one is given a labeled set of observations $\textbf{X}_\ell = \{\textbf{x}_1, \textbf{x}_2, ..., \textbf{x}_{|\textbf{X}_\ell|}\}$~\footnote{We will henceforth use the notation $|S|$ to represent the size of a given set $S$.} together with their class correspondences 
$\text{Y} = \{y_1, y_2, ..., y_{|\text{Y}|}\}$, taken from the joint distribution $P(\textbf{X}, \text{Y})$. The goal is to 
find a function that maps any given observation to a class, $\textbf{x} \rightarrow y_i \in \text{Y}$, such that 
the classification error is minimized.\vertbreak

Considering an SSL setting, the classification 
problem is similar --- a labeled set $\textbf{X}_\ell$ is also given, the goal is to estimate a 
relation $\textbf{x} \rightarrow y_i \in \text{Y}$ --- but now with the addition of an 
unlabeled set $\textbf{X}_u = \{\textbf{x}'_1, \textbf{x}'_2, ..., \textbf{x}'_{|\textbf{X}_u|}\}$ (i.e. without class label correspondences), where 
usually $|\textbf{X}_u| \gg |\textbf{X}_\ell|$, with samples now taken from 
the marginal distribution $P(\textbf{X})$. Starting with assumptions about the 
distribution of the data $P(\textbf{X})$ (e.g. treating it as a mixture distribution), SSL methods tackle the problem 
by supplying $\textbf{X}_u$ to unsupervised learning techniques, in order to 
reach good estimates of `hidden' (also referred to as `latent') class-conditional probabilities 
$P(\textbf{X}|\text{Z})$. The difference is that 
now the number and type of cases for Z are pre-determined by the 
class correspondences of the labeled dataset, $\text{Y}$. The assignment of 
each `latent' component of the distribution to elements in $\text{Y}$ can then be guided with 
the help of the labeled dataset, $\textbf{X}_\ell$.\vertbreak 

The success of 
SSL techniques is heavily dependent on the model's 
assumptions about the distribution of $P(\textbf{X})$, with severe negative impact on 
performance in the case of inappropriate model-to-problem 
matchings~\cite{zhu2009introduction,zhu05survey}. 
Chapelle et al.~\cite{chapelle2010semi} list the key types of assumptions often 
made by SSL methods:

\begin{itemize}

    \item \textbf{Smoothness:} If two points $x_1$ and $x_2$ are close to each other in 
            a high density region, then so should be the respective labels 
            $y_1$ and $y_2$, i.e. the labeling function is assumed to be 
            \textit{smoother} in high density regions.
    \item \textbf{Clustering or low density separation:} Points belonging to the same cluster are likely 
            to belong to the same class. Following a somewhat complementing idea to 
            `smoothness', decision boundaries should 
            lie in low-density regions.
    \item \textbf{Manifold structures}~\cite{chapelle2010semi}
    \item \textbf{Transduction}~\cite{chapelle2010semi}

\end{itemize}

% it would be great to post the assumptions in front of the methods or special 
% cases of the method classes
Several classes of SSL methods exist, each considering specialized cases of one 
(or more) of the 
aforementioned assumptions: Self-Training, Co-Training~\cite{zhu05survey}, Generative Models,
Low-Density Separation methods, Graph-Based Methods~\cite{chapelle2010semi}, among 
others~\cite{zhu05survey}.

\subsection{Semi-Supervised Generative Models}
\label{subsec:gen-models}

In their way to estimate 
$P(\text{Y}|\textbf{X})$, Generative Models (GMs) first find a $y_i \rightarrow \textbf{x}$ mapping by modeling the 
class-conditional distributions $P(\textbf{X}|\text{Y})$, instead of directly going for 
an estimation of the posteriors. In other words, GMs go through the `trouble' of 
estimating how data is generated by each element in Y 
in order to classify an observation.\vertbreak 

GMs treat $P(\textbf{X}|\text{Y})$ as 
a mixture model, where components belong to a 
family of parametric distributions (e.g. Gaussian, Multinomial, etc.), governed 
by a set of parameters 
$\theta = \{\theta_1, \theta_2, ..., \theta_n\}$. Unsupervised learning methods 
are applied over the unlabeled data $\textbf{X}_u$ in order to find estimates 
for those parameters, $\hat{\theta}$~\footnote{We represent estimations of a variable $x$ by 
topping it with an ` $\hat{}$ ' sign, $\hat{x}$.}, and identify the 
components of the mixture model, usually with a 1:1 correspondence between 
the `latent' clusters and each class in Y (Nigam et al.~\cite{Nigam2000} 
propose both 1:1 and N:1 correspondence alternatives, Zhu~\cite{zhu05survey} also points 
out that such assumptions should be carefully considered in order not to 
incur in model incorrectness). The posterior 
probabilities $P(\text{Y} = y_i|\textbf{X} = \textbf{x}, \hat{\theta})$ can then be determined via Bayes' Rule:
\[P(y_i|\textbf{x}, \hat{\theta}) = \frac{P(y_i|\hat{\theta})P(\textbf{x}|y_i,\hat{\theta})}{\sum_{i=1}^{|\text{Y}|}P(y_i|\hat{\theta})P(y_i|\textbf{x},\hat{\theta})}\]

The training of a classifier consists in 
determining the parameter estimates that maximize the likelihood 
(Maximum Likelihood estimation) of both 
labeled and unlabeled data: 
\begin{equation}
    \prod_{\textbf{x}_i \in \textbf{X}_\ell}P(y_i)P(\textbf{x}_i|y_i,\theta) \times \prod_{\textbf{x}_i \in \textbf{X}_u}\sum_{k=1}^{|\text{Y}|}P(y_k)P(\textbf{x}_i|y_k,\theta)
    \label{eq:mle}
\end{equation}

or via Maximum a Posteriori (MAP) estimation of 
$P(\theta|\textbf{X}_\ell,\textbf{X}_u)$. These estimates are usually tackled 
via gradient descent methods~\cite{chapelle2010semi} or using Expectation Maximization (EM) 
algorithms~\cite{Nigam2000}, as with the case described in Section~\ref{subsec:semi-super-em}.


