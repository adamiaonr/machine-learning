\subsection{Text Classification via Expectation Regularization (XR)}

A. McCallum, one of the authors of~\cite{Nigam2000,Nigam2010} and original 
advocates of the application of EM to text classification, proposed in 2007 a 
different approach for the same task, based on Expectation Regularization 
(XR)~\cite{Mann2007a}.\vertbreak

(...) specifically applied to Multinomial Logistic Regression (MLG) models, also 
known as SoftMax Regression~\cite{}. In such type of models, the probability of 
a class $c_j$ given an article $a_i$ (and set of estimated model parameters 
$\hat{\theta}$) is given by the expression:

\begin{equation}
\begin{split}
    P(c_j|a_i,\theta) &= \frac{\text{exp}\left(\theta_j a_i\right)}{\sum_{m=1}^{M}\text{exp}\left(\theta_m a_i\right)}
    \label{eq:posterior-log-reg}
\end{split}
\end{equation}

where (...). Since this work is part of a (...), details on the estimation 
of the $\theta$ parameters are given in Section~\ref{subsubsec:log-reg-params}. 

\subsubsection{Estimation of $\theta$ Parameters for MLG}
\label{subsubsec:log-reg-params}

\footnotesize
\begin{equation}
\begin{split}
    J(\hat{\theta}) &= -\frac{1}{A}\left[\sum_{i=1}^{A}\sum_{j=1}^{C}1\{c_{i} = c_{j}\}\text{log}\left(\frac{e^{\theta_j a_i}}{\sum_{m=1}^{M}e^{\theta_m a_i}}\right)\right]
    \label{eq:log-reg-cost}
\end{split}
\end{equation}
\normalsize

As there is no known closed-form to estimate $\hat{\theta}$ which minimize 
expression~\ref{eq:log-reg-cost}, an iterative algorithm --- e.g. gradient 
descent~\cite{} --- must be applied. In order to use gradient descent, one must 
find an expression for the partial derivative (with respect to each $\theta_j$) 
of the cost function $J$:

\footnotesize
\begin{equation}
\begin{split}
    \nabla_{\theta_j}J(\hat{\theta}) &=\\ 
    &-\frac{1}{A}\sum_{i=1}^{A}\left[a_i\left(1\{c_{i} = c_{j}\} - \left(\frac{e^{\theta_j a_i}}{\sum_{m=1}^{M}e^{\theta_m a_i}}\right)\right)\right]
    \label{eq:part-der-log-reg-cost}
\end{split}
\end{equation}
\normalsize

The expression~\ref{eq:part-der-log-reg-cost} can then be applied within a 
batch gradient descent algorithm such as:

(...).

SSL methods based on Generative approaches attempt to model the 
class-conditional probabilities $P(x|y_n)$ and priors $P(y_n)$, for each 
class $y_n$ with $n \in \{1, 2, ..., N\}$. Using Bayes' theorem, the posteriors 
can then be determined as:
\[P(y_n|x) = \frac{P(y_n)P(x|y_n)}{\sum_{n=1}^{N}P(y_n)P(y_n|x)}\]

In the case of GMs, assumptions are made about the nature of the distributions 
$P(x|y_n)$


Within the scope of SSL, GMs can be seen from both supervised and unsupervised 
points of view: 




the data $x$ is 
generated by a combination of $N$ `basic' distributions (e.g. Gaussians in the 
case of Gaussian Mixture Models), often designated by mixture components. Each 
one of the components is assumed to be associated with a class $y_n$ of 
the classification problem, modeling the 
distribution $P(x|y_n)$, i.e. the probability of the data given a class. 
The distributions $P(x|y_n)$ are assumed to be governed by a set of parameters 
$\theta = \{\theta_1, \theta_2, ..., \theta_N\}$, being often represented as 
$P(x|y_n, \theta_n)$. The training phase of a generative model consists in 
determining the parameters $\theta$ via Maximum a Posteriori (MAP) estimation, 
i.e. finding $\theta$ which maximize the probability of the 
parameters given the data, $P(\theta|x)$. With the estimation of 
$P(x|y_n, \theta_n)$ and the priors $P(y_n|\theta)$

(...) as a combination of clustering and classification 

%(...) `bag-of-words' approach. 

(...), let an article $a$, part of a set of articles $\mathcal{A}$ and 
belonging to a topic (i.e. a label\slash class) $t$, be represented by an 
array $a = \{w_{1}, w_{2}, ..., w_{n}, t\}$, i.e. a list of $n$ features 
and its label. Each feature $w_i$, with $i \in \{1,2, ..., n\}$, is related to a 
word contained in the article $a$. Although it is not our intention to specify 
the (...) at this 
point (...), in text classification problems $w_i$ typically corresponds to the 
number of occurrences of a specific word in an article, although other 
approaches such as Term Frequency and Inverse Document Frequency 
(TF-IDF)~\cite{Rennie03tacklingthe}, explained thoroughly in 
Section~\ref{sec:exp-setup}.\vertbreak

(...) the set of unique words in an article forms a dictionary $\mathcal{D}$. 
Since we will be dealing with unlabeled articles during semi-supervised 
training phases, it should be noticed that $\mathcal{D}$ includes words from 
both labeled and unlabeled data.\vertbreak

%\footnote{Before the (...) phase, an article 
%$a$ may be stripped of certain words --- e.g. newsgroup-related metadata in the 
%case of the \textbf{20 Newsgroups} dataset~\cite{} --- to avoid (...)}.\vertbreak

(...) similarly to the notations used in~\cite{Nigam2000,Su2011} the size in 
words of an article $a$ is represented as $|a|$ and 


The authors propose the extension of the MNB text 
classifier with Expected 
Maximization (EM) algorithms to perform MAP estimations of the mixture model 
parameters $\theta$ considering unlabeled data, 
arguing that such extension may deliver better results when considering a 
combined dataset with small amounts of labeled 
data vs. large amounts of unlabeled data.\vertbreak

Besides using integer word counts as features (typical of `bag of words' 
approaches) we also test our implementations with features converted by the 
TF-IDF~\cite{Rennie03tacklingthe} transformation:

\begin{equation}
    \text{tf-idf}(w_k) = \text{log}(N_{i,k} + 1) + \text{log}(\frac{|\mathcal{A}|}{|\mathcal{A}_{w_k}|})
    \label{eq:tf-idf}
\end{equation}

where $N_{i,k}$ is the number of times the word $w_k$ appears in an article 
$a_i$ and $|\mathcal{A}_{w_k}|$ is the number of articles containing the word 
$w_k$. We have decided for its evaluation following an insight by 
previous work~\cite{Rennie03tacklingthe,Kibriya:2004:MNB:2146834.2146882}, 
which mention potential benefits of TF-IDF.
